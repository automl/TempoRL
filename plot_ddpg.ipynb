{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "from utils.plotting import get_colors, load_config, plot\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sb\n",
    "\n",
    "from scipy.signal import savgol_filter\n",
    "    \n",
    "\n",
    "# Somehow the plotting functionallity I ended up with was already covered for the tabular case.\n",
    "# I should have just used the plot function from that.\n",
    "def plotMultiple(data, ylim=None, title='', logStepY=False, max_steps=200, xlim=None, figsize=None,\n",
    "                 alphas=None, smooth=5, savename=None, rewyticks=None, lenyticks=None,\n",
    "                 skip_stdevs=[], dont_label=[], dont_plot=[], min_steps=None):\n",
    "    \"\"\"\n",
    "    Simple plotting method that shows the test reward on the y-axis and the number of performed training steps\n",
    "    on the x-axis.\n",
    "    \n",
    "    data -> (dict[agent name] -> list([rewards, lens, decs, train_steps, train_episodes])) the data to plot\n",
    "    ylim -> (list) y-axis limit\n",
    "    title -> (str) title on top of plot\n",
    "    logStepY -> (bool) flag that indicates if the y-axis should be on log scale.\n",
    "    max_steps -> (int) maximal episode length\n",
    "    xlim -> (list) x-axis limits\n",
    "    figsize -> (list) dimensions of the figure\n",
    "    alphas -> (dict[agent name] -> float) the alpha value to use for plotting of specific agents\n",
    "    smooth -> (int) the window size for smoothing (has to be odd if used. < 0 deactivates this option)\n",
    "    savename -> (str) filename to save the figure\n",
    "    rewyticks -> (list) yticks for the reward plot\n",
    "    lenyticks -> (list) yticks for the decisions plot\n",
    "    skip_sdevs -> (list) list of names to not plot standard deviations for.\n",
    "    dont_label -> (list) list of names to not label.\n",
    "    dont_plot -> (list) list of names to not plot.\n",
    "    \"\"\"\n",
    "    \n",
    "    if smooth and smooth > 0:\n",
    "        degree = 2\n",
    "        for agent in data:\n",
    "            data[agent] = list(data[agent])  # we have to convert the tuple to lists\n",
    "            data[agent][0] = list(data[agent][0])\n",
    "            data[agent][0][0] = savgol_filter(data[agent][0][0], smooth, degree)  # smooth the mean reward\n",
    "            data[agent][0][1] = savgol_filter(data[agent][0][1], smooth, degree)  # smooth the stdev reward\n",
    "            data[agent][1] = list(data[agent][1])\n",
    "            data[agent][1][0] = savgol_filter(data[agent][1][0], smooth, degree)  # smooth mean num steps\n",
    "            data[agent][1][1] = savgol_filter(data[agent][1][1], smooth, degree)\n",
    "            data[agent][2] = list(data[agent][2])\n",
    "            data[agent][2][0] = savgol_filter(data[agent][2][0], smooth, degree)  # smooth mean decisions\n",
    "            data[agent][2][1] = savgol_filter(data[agent][2][1], smooth, degree)\n",
    "\n",
    "    colors, color_map = get_colors()\n",
    "    \n",
    "\n",
    "    cfg = load_config()\n",
    "    sb.set_style(cfg['plotting']['seaborn']['style'])\n",
    "    sb.set_context(cfg['plotting']['seaborn']['context']['context'],\n",
    "                   font_scale=cfg['plotting']['seaborn']['context']['font scale'],\n",
    "                   rc=cfg['plotting']['seaborn']['context']['rc2'])\n",
    "\n",
    "    if figsize:\n",
    "        fig, ax = plt.subplots(2, figsize=figsize, dpi=100, sharex=True)\n",
    "    else:\n",
    "        fig, ax = plt.subplots(2, figsize=(20, 10), dpi=100,sharex=True)\n",
    "    ax[0].set_title(title)\n",
    "\n",
    "    for agent in list(data.keys())[::-1]:\n",
    "        if agent in dont_plot:\n",
    "            continue\n",
    "        try:\n",
    "            alph = alphas[agent]\n",
    "        except:\n",
    "            alph = 1.\n",
    "        color_name = None\n",
    "        if 'dar' in agent:\n",
    "            color_name = color_map['dar']\n",
    "        elif agent.startswith('t'):\n",
    "            color_name = color_map['t-DDPG']\n",
    "        elif agent.startswith('f'):\n",
    "            color_name = color_map['f-DDPG']\n",
    "        else:\n",
    "            color_name = color_map[agent]\n",
    "        rew, lens, decs, train_steps, train_eps = data[agent]\n",
    "        \n",
    "        label = agent.upper()\n",
    "        if agent.startswith('t'):\n",
    "            label = 't-DDPG'\n",
    "        elif agent.startswith('f'):\n",
    "            label = 'FiGAR'\n",
    "        elif agent.startswith('e'):\n",
    "            label = r'$\\epsilon$z-DQN'\n",
    "        elif agent in dont_label:\n",
    "            label = None\n",
    "\n",
    "        #### Plot rewards\n",
    "        ax[0].step(train_steps[0], rew[0], where='post', c=colors[color_name], label=label,\n",
    "                   alpha=alph)\n",
    "        if agent not in skip_stdevs:\n",
    "            ax[0].fill_between(train_steps[0], rew[0]-rew[1], rew[0]+rew[1], alpha=0.25 * alph, step='post',\n",
    "                               color=colors[color_name])\n",
    "        #### Plot lens\n",
    "        ax[1].step(train_steps[0], decs[0], where='post', c=np.array(colors[color_name]), ls='-',\n",
    "                   alpha=alph)\n",
    "        if agent not in skip_stdevs:\n",
    "            ax[1].fill_between(train_steps[0], decs[0]-decs[1], decs[0]+decs[1], alpha=0.125 * alph, step='post',\n",
    "                               color=np.array(colors[color_name]))\n",
    "        ax[1].step(train_steps[0], lens[0], where='post',\n",
    "                   c=np.array(colors[color_name]) * .75, alpha=alph, ls=':')\n",
    "        \n",
    "        if agent not in skip_stdevs:\n",
    "            ax[1].fill_between(train_steps[0], lens[0]-lens[1], lens[0]+lens[1], alpha=0.25 * alph, step='post',\n",
    "                               color=np.array(colors[color_name]) * .75)\n",
    "    ax[0].semilogx()\n",
    "    if rewyticks is not None:\n",
    "        ax[0].set_yticks(rewyticks)\n",
    "    if ylim:\n",
    "        ax[0].set_ylim(ylim)\n",
    "    if xlim:\n",
    "        ax[0].set_xlim(xlim)\n",
    "    ax[0].set_ylabel('Reward')\n",
    "    if len(data) - len(dont_label) < 5:\n",
    "        ax[0].legend(ncol=1, loc='best', handlelength=.75)\n",
    "    ax[1].semilogx()\n",
    "    if logStepY:\n",
    "        ax[1].semilogy()\n",
    "        \n",
    "    ax[1].plot([-999, -999], [-999, -999], ls=':', c='k', label='all')\n",
    "    ax[1].plot([-999, -999], [-999, -999], ls='-', c='k', label='dec')\n",
    "    ax[1].legend(loc='best', ncol=1, handlelength=.75)\n",
    "    ax[1].set_ylim([min_steps if min_steps is not None else 1, max_steps])\n",
    "    if xlim:\n",
    "        ax[1].set_xlim(xlim)\n",
    "    ax[1].set_ylabel('#Actions')\n",
    "    ax[1].set_xlabel('#Train Steps')\n",
    "    if lenyticks is not None:\n",
    "        ax[1].set_yticks(lenyticks)\n",
    "    plt.tight_layout()\n",
    "    if savename:\n",
    "        plt.savefig(savename)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "results = {}\n",
    "ddpg_datas = []\n",
    "for i in sorted(os.listdir('experiments/ddpg/DDPG')):\n",
    "    ddpg_datas.append(np.load(f'experiments/ddpg/DDPG/{i}/DDPG_Pendulum-v0_{i}.npy'))\n",
    "\n",
    "\n",
    "ddpg_mean = np.mean(ddpg_datas, axis=0)\n",
    "ddpg_stdev = np.std(ddpg_datas, axis=0)\n",
    "results['DDPG'] = [[ddpg_mean[:, 1], ddpg_stdev[:, 1]],\n",
    "                   [ddpg_mean[:, 3], ddpg_stdev[:, 3]], \n",
    "                   [ddpg_mean[:, 2], ddpg_stdev[:, 2]],\n",
    "                   [ddpg_mean[:, 0], ddpg_mean[:, 0]],\n",
    "                   [ddpg_mean[:, 0], ddpg_mean[:, 0]]]\n",
    "\n",
    "for max_len in [2, 4, 6, 8, 10, 12, 14, 16, 18, 20]:\n",
    "    temporl_datas = []\n",
    "    for i in sorted(os.listdir(f'experiments/ddpg/TempoRLDDPG/{max_len}')):\n",
    "        temporl_datas.append(np.load(f'experiments/ddpg/TempoRLDDPG/{max_len}/{i}/TempoRLDDPG_Pendulum-v0_{i}.npy'))\n",
    "\n",
    "    figar_datas = []\n",
    "    for i in sorted(os.listdir(f'experiments/ddpg/FiGARDDPG/{max_len}')):\n",
    "        figar_datas.append(np.load(f'experiments/ddpg/FiGARDDPG/{max_len}/{i}/FiGARDDPG_Pendulum-v0_{i}.npy'))\n",
    "\n",
    "    temporl_mean = np.mean(temporl_datas, axis=0)\n",
    "    figar_mean = np.mean(figar_datas, axis=0)\n",
    "    temporl_stdev = np.std(temporl_datas, axis=0)\n",
    "    figar_stdev = np.std(figar_datas, axis=0)\n",
    "    \n",
    "    # (dict[agent name] -> list([rewards, lens, decs, train_steps, train_episodes]))\n",
    "    results['t-DDPG'] = [[temporl_mean[:, 1], temporl_stdev[:, 1]],\n",
    "                         [temporl_mean[:, 3], temporl_stdev[:, 3]],\n",
    "                         [temporl_mean[:, 2], temporl_stdev[:, 2]],\n",
    "                         [temporl_mean[:, 0], temporl_mean[:, 0]],\n",
    "                         [temporl_mean[:, 0], temporl_mean[:, 0]]]\n",
    "    results['f-DDPG'] = [[figar_mean[:, 1], figar_stdev[:, 1]],\n",
    "                         [figar_mean[:, 3], figar_stdev[:, 3]],\n",
    "                         [figar_mean[:, 2], figar_stdev[:, 2]],\n",
    "                         [figar_mean[:, 0], figar_mean[:, 0]],\n",
    "                         [figar_mean[:, 0], figar_mean[:, 0]]]\n",
    "    print(min(min(results['DDPG'][0][0]), min(results['t-DDPG'][0][0]), min(results['f-DDPG'][0][0])),\n",
    "          max(max(results['DDPG'][0][0]), max(results['t-DDPG'][0][0]), max(results['f-DDPG'][0][0])))\n",
    "    print('  DDPG AUC:', np.mean((results['DDPG'][0][0] + 1800) / (-145 + 1800)))\n",
    "    print('t-DDPG AUC:', np.mean((results['t-DDPG'][0][0] + 1800) / (-145 + 1800)))\n",
    "    print(' FiGAR AUC:', np.mean((results['f-DDPG'][0][0] + 1800) / (-145 + 1800)))\n",
    "    plotMultiple(results, title=r'Pendulum-v0  -- $\\mathcal{J}=' + f'{max_len}$',\n",
    "                 smooth=0, ylim=[-1800, -50], min_steps=10, max_steps=210, xlim=[10**3, 3*10**4],\n",
    "                 lenyticks=[50, 125, 200], rewyticks=[-1800, -1000, -200],\n",
    "                 savename=f'ddpg_{max_len}.pdf')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
